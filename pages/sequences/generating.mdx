import { Callout } from "nextra/components";

# Generating Tokens

Use the `gen` operation to ask the model to generate tokens in its context window: 

```typescript file=gen.ts
await seq.append("What are 3 fun things to do in San Francisco?");

// generate a response
const response = await seq.gen().text();
console.log(response);
```

## Stopping

You can instruct the model to stop generating tokens by using the `stopAt` or `limit` options. 

### Token count 

Stop generation after a certain number of tokens by using the `limit` option: 

```typescript file=limit.ts
await seq.append("What's the meaning of life?");

// generate a response
const response = await seq.gen({ limit: 10 }).text();
console.log(response);
```

### Stop text

Stop generation when a fragment of text is generated by using the `stopAt` option: 

```typescript file=stop-at.ts
await seq.append("What's the meaning of life?");

// generate a response
const response = await seq.gen({ stopAt: "\n\n" }).text();
console.log(response);
```

## Sampling 

Mixlayer supports a variety of sampling methods, including: 

- `temperature`: sample from the distribution with a temperature parameter
- `topP`: nucleus sampling, select the most likely tokens up to a cumulative probability threshold
- `topK`: limit sampling to the top K highest probability tokens
- `seed`: set a fixed random seed for reproducibility
- `repeatPenalty`: penalize the model for repeating text

<Callout type="info">
  Different models require different sampling parameters for optimal results, check the model's documentation for more information. 
</Callout>

```typescript file=sampling.ts
await seq.append("What's the meaning of life?");

// generate a response
const response = await seq.gen({ 
  topP: 0.9, // nucleus sampling
  seed: 42, // reproducibility
  repeatPenalty: 1.2, // penalize the model for repeating text
  temperature: 0.7, // sample from the distribution with a temperature parameter
  topK: 10, // limit sampling to the top K highest probability tokens
  repeatPenalty: 1.2, // penalize the model for repeating text
}).text();
console.log(response);
```

If no sampling method is specified, the model will use greedy sampling by default (select the highest probability token). 

## Thinking

If you're using a reasoning model, you can instruct the model to think about its response by using the `thinking` option: 

```typescript file=thinking.ts
// open a seq on a reasoning model
const seq = await socket.open("qwen/qwen3-8b");
await seq.append("What's the meaning of life?\n\n", { role: "user" });

// capture thinking text
const thinkingText = await seq
  .gen({ thinking: true, stopAt: "</think>", role: "assistant" })
  .text();

console.log("thoughts: \n", thinkingText);

// generate a response after thinking a bit
const response = await seq.gen({ role: "assistant" }).text();
console.log("\n---\nresponse: \n", response);
```

This will automatically prefill the model's response with a `<think>` tag. For hybrid models, you can set `thinking: false` to ask the model to skip its thinking phase. 

## Streaming 

Mixlayer text genearation is natively streaming, so you can process or forward the model's response as it's being generated. Instead of using the `text` method, you can use the `textStream` method to get a stream of text chunks:

```typescript file=streaming.ts
const seq = await socket.open("meta/llama3.1-8b-instruct-free");

await seq.append("You are a helpful assistant that talks like a pirate.", 
   { role: "system" });

await seq.append("What's so dangerous about Drake's passage?", 
   { role: "user" });

// generate a response using the textStream method
const stream = await seq.gen({ role: "assistant" }).textStream();

for await (const chunk of stream) {
  process.stdout.write(chunk);
}
```
